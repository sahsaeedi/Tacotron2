{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextToSpeech.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfHapC3SfOYa"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdCUAjonsDZK"
      },
      "source": [
        "from tensorflow.keras import initializers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DKMPXC8sOBK"
      },
      "source": [
        "class Linear(tf.Module):\n",
        "  def __init__(self, input_dim: int, output_dim: int, bias: bool = True):\n",
        "    super(Linear, self).__init__()\n",
        "    #self.init = initializers.GlorotUniform()\n",
        "    #self.weights = tf.Variable(self.init([input_dim, output_dim]))\n",
        "    self.weights = tf.Variable(tf.random.normal([input_dim, output_dim]))\n",
        "    self.bias = tf.Variable(tf.zeros([output_dim]))\n",
        "    #print(\"input \", input_dim)\n",
        "    #print(\"output : \", output_dim)\n",
        "    #print(\"weights : \", self.weights.shape)\n",
        "    if bias:\n",
        "      self.bias = tf.Variable(tf.zeros([output_dim]))\n",
        "      \n",
        "\n",
        "  def __call__(self, x):\n",
        "    #print(tf.matmul(x, self.weights).shape)\n",
        "    #print(\"x : \", x.shape)\n",
        "    #print(\"weights : \", self.weights.shape)\n",
        "    #print(self.bias.shape)\n",
        "    y = tf.matmul(x, self.weights) + self.bias\n",
        "\n",
        "    return tf.nn.relu(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EMPi5iyWY8c"
      },
      "source": [
        "class Linear2(tf.keras.Model):\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super(Linear2, self).__init__()\n",
        "    self.input = tf.keras.layers.Input((output_dim,))\n",
        "    self.linear = tf.keras.layers.Dense(units=output_dim)\n",
        "                    \n",
        "    \n",
        "  def call(self, x):\n",
        "    x = self.input(x)\n",
        "    x = self.linear(x)\n",
        "    return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSDoX7G3FoZB"
      },
      "source": [
        "class ConvBlock(tf.Module):\n",
        "  def __init__(self,\n",
        "               input_dim: int,\n",
        "               output_dim: int,\n",
        "               kernel_size: int,\n",
        "               padding: str,\n",
        "               dropout_p: float = 0.5,\n",
        "               activation = 'relu') -> None:\n",
        "\n",
        "    super(ConvBlock, self).__init__()\n",
        "\n",
        "    self.conv = tf.keras.models.Sequential([\n",
        "                                            #tf.keras.layers.Input(shape=(input_dim, output_dim)),\n",
        "                                            tf.keras.layers.Conv1D(output_dim, kernel_size= kernel_size, strides= 1, padding= padding),\n",
        "                                            tf.keras.layers.BatchNormalization(),\n",
        "                                            tf.keras.layers.ReLU(),\n",
        "                                            tf.keras.layers.Dropout(rate= dropout_p)\n",
        "\n",
        "    ])\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return self.conv(inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pa4EiHoqfaaL"
      },
      "source": [
        "class PreNet(tf.keras.Model):\n",
        "  def __init__(self, input_dim: int, output_dim: int, dropout_p: float) -> None:\n",
        "    super(PreNet, self).__init__()\n",
        "\n",
        "    self.fully_connectd_layers = tf.keras.models.Sequential([\n",
        "                                                             Linear(input_dim, output_dim),\n",
        "                                                             tf.keras.layers.ReLU(),\n",
        "                                                             tf.keras.layers.Dropout(rate= dropout_p),\n",
        "                                                             Linear(output_dim, output_dim),\n",
        "                                                             tf.keras.layers.ReLU(),\n",
        "                                                             tf.keras.layers.Dropout(rate= dropout_p)\n",
        "    ])\n",
        "\n",
        "  def call(self, input):\n",
        "    return self.fully_connectd_layers(input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkMeP-xWiuGs"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self,\n",
        "               vocab_size:int,\n",
        "               embedding_dim: int = 512,\n",
        "               encoder_lstm_dim: int = 256,\n",
        "               num_lstm_layers: int = 1,\n",
        "               conv_dropout_p: float = 0.5,\n",
        "               num_conv_layers: int = 3,\n",
        "               conv_kernel_size: int = 5) -> None:\n",
        "\n",
        "    super(Encoder, self).__init__()\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.convs_layers = tf.keras.models.Sequential([\n",
        "                                                    ConvBlock(embedding_dim,\n",
        "                                                              embedding_dim,\n",
        "                                                              kernel_size = conv_kernel_size,\n",
        "                                                              padding = 'valid',\n",
        "                                                              dropout_p = conv_dropout_p\n",
        "                                                              ) for _ in range(num_conv_layers)\n",
        "               ])\n",
        "    self.lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(num_lstm_layers))\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "  def call(self, inputs, input_lenghts):\n",
        "    inputs = self.embedding(inputs)\n",
        "    #inputs = inputs.transpose(1, 2)\n",
        "    #print(inputs.shape)\n",
        "    inputs = tf.transpose(inputs, perm=[1,2,0])\n",
        "\n",
        "    inputs = self.convs_layers(inputs)\n",
        "    inputs = tf.transpose(inputs, perm=[1,2,0])\n",
        "\n",
        "    output= self.lstm(inputs)\n",
        "    return output\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAdw_GTHMHFb"
      },
      "source": [
        "    #self.lstm = tf.keras.Model.Sequential([\n",
        "    #                                       tf.keras.layers.Input(shape= (embedding_dim, num_lstm_layers)),\n",
        "    #                                       tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(num_lstm_layers))\n",
        "    #])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdFXe9ztoMqy"
      },
      "source": [
        "class LocationSensitiveAtention(tf.keras.Model):\n",
        "  def __init__(self,\n",
        "               lstm_hidden_dim: int = 1024,\n",
        "               embedding_dim: int = 512,\n",
        "               attn_dim: int = 128,\n",
        "               location_conv_filter_size: int = 32,\n",
        "               location_conv_kernel_size: int = 31) -> None:\n",
        "\n",
        "    super(LocationSensitiveAtention, self).__init__()\n",
        "    self.attn_dim = attn_dim\n",
        "    #self.query_proj = Linear(lstm_hidden_dim, attn_dim, bias= False)\n",
        "    self.query_proj = Linear2(lstm_hidden_dim, attn_dim)\n",
        "    #self.value_proj = Linear(embedding_dim, attn_dim, bias = False)\n",
        "    self.value_proj = Linear2(embedding_dim, attn_dim)\n",
        "    #self.align_proj = Linear(attn_dim, 1, bias = True)\n",
        "    self.align_proj = Linear2(attn_dim, 1)\n",
        "\n",
        "    self.bias = tf.Variable(tf.random.uniform(shape= (attn_dim,), minval= -0.1, maxval= 0.1))\n",
        "\n",
        "    self.location_conv = tf.keras.layers.Conv1D(filters= location_conv_filter_size,\n",
        "                                                kernel_size = location_conv_kernel_size,\n",
        "                                                use_bias = False\n",
        "                                                )\n",
        "    self.location_proj = Linear(location_conv_filter_size, attn_dim, bias= False)\n",
        "  \n",
        "  def call(self, query, value, last_alignment):\n",
        "    batch_size = query.shape[0]\n",
        "    print(query.shape)\n",
        "    query = tf.expand_dims(query, 1)\n",
        "    print(query.shape)\n",
        "    #query = query.unsqueeze(1)\n",
        "\n",
        "    last_alignment = self.location_conv(last_alignment)\n",
        "    last_alignment = tf.transpose(last_alignment, perm= [0,1,2])\n",
        "    last_alignment = self.location_proj(last_alignment)\n",
        "    #print(tf.reshape(query, shape=[-1, query.shape[2]]).shape)\n",
        "    #print(self.query_proj(tf.reshape(query, shape=[-1, query.shape[2]])))\n",
        "    #alignment = tf.squeeze(self.align_proj(tf.math.tanh(\n",
        "    #    tf.reshape(self.query_proj(tf.reshape(query, shape=[-1, query.shape[2]])), shape= [batch_size, -1, self.attn_dim])\n",
        "    #    + tf.reshape(self.value_proj(tf.reshape(value, shape=[-1, value.shape[2]])), shape= [batch_size, -1, self.attn_dim])\n",
        "    #    + last_alignment\n",
        "    #    + self.bias\n",
        "    #)), axis = -1)\n",
        "\n",
        "    #alignment = tf.nn.softmax(alignment, axis= -1)\n",
        "\n",
        "    #context = tf.matmul(tf.squeeze(alignment, 1), value)\n",
        "    #context = tf.squeeze(context, 1)\n",
        "\n",
        "    #return context, alignment\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhk57Z8cLv5s"
      },
      "source": [
        "batch_size = 3\n",
        "seq_length = 100\n",
        "query_dim = 1024\n",
        "value_dim = 512\n",
        "align_dim = 2\n",
        "\n",
        "query = tf.random.uniform(shape= (batch_size, 1, query_dim), minval= -0.01, maxval= 0.01)\n",
        "value = tf.random.uniform(shape= (batch_size, seq_length, value_dim), minval= -0.01, maxval= 00.1)\n",
        "align = tf.random.uniform(shape= (batch_size, seq_length, align_dim), minval= -0.01, maxval= 0.01)\n",
        "\n",
        "attention = LocationSensitiveAtention()\n",
        "output = attention(query, value, align)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ybn24mFoTMT"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self,\n",
        "               num_mel_bins: int = 80,\n",
        "               prenet_dim: int = 256,\n",
        "               decoder_lstm_dim: int = 1024,\n",
        "               attn_lstm_dim: int = 1024,\n",
        "               embedding_dim: int = 512,\n",
        "               attn_dim: int = 128,\n",
        "               location_conv_filter_size: int = 32,\n",
        "               location_conv_kernel_size: int = 31,\n",
        "               prenet_dropout_p: float = 0.5,\n",
        "               attn_dropout_p: float = 0.1,\n",
        "               decoder_dropout_p: float = 0.1,\n",
        "               max_decoding_step: int = 1000,\n",
        "               stop_threshold: float = 0.5) -> None:\n",
        "\n",
        "    \n",
        "    super(Decoder, self).__init__()\n",
        "    self.num_mel_bins = num_mel_bins\n",
        "    self.max_decoding_step = max_decoding_step\n",
        "    self.decoder_lstm_dim = decoder_lstm_dim\n",
        "    self.attn_lstm_dim = attn_lstm_dim\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.attn_dropout_p = attn_dropout_p\n",
        "    self.decoder_dropout_p = decoder_dropout_p\n",
        "    self.stop_threshold = stop_threshold\n",
        "\n",
        "    self.prenet = PreNet(self.num_mel_bins, prenet_dim, prenet_dropout_p)\n",
        "    self.lstm = [tf.keras.layers.LSTMCell(attn_lstm_dim),\n",
        "                 tf.keras.layers.LSTMCell(decoder_lstm_dim)]\n",
        "\n",
        "    \n",
        "    self.attention = LocationSensitiveAtention(\n",
        "        lstm_hidden_dim = decoder_lstm_dim,\n",
        "        embedding_dim = embedding_dim,\n",
        "        attn_dim = attn_dim,\n",
        "        location_conv_filter_size = location_conv_filter_size,\n",
        "        location_conv_kernel_size = location_conv_kernel_size\n",
        "    )\n",
        "\n",
        "    self.mel_generator = Linear(decoder_lstm_dim + embedding_dim, num_mel_bins)\n",
        "    self.stop_generator = Linear(decoder_lstm_dim + embedding_dim, 1)\n",
        "\n",
        "\n",
        "  def _init_decoder_states(self, encoder_outputs):\n",
        "\n",
        "    lstm_outputs = list()\n",
        "    lstm_hiddens = list()\n",
        "\n",
        "    batch_size =encoder_outputs.shape[0]\n",
        "    seq_length = encoder_outputs.shape[1]\n",
        "\n",
        "    lstm_outputs.append(tf.zeros(shape = [batch_size, self.attn_lstm_dim]))\n",
        "    lstm_outputs.append(tf.zeros(shape = [batch_size, self.decoder_lstm_dim]))\n",
        "\n",
        "\n",
        "    lstm_hiddens.append(tf.zeros(shape = [batch_size, self.attn_lstm_dim]))\n",
        "    lstm_hiddens.append(tf.zeros(shape = [batch_size, self.decoder_lstm_dim]))\n",
        "\n",
        "\n",
        "    alignment = tf.zeros(shape= [batch_size, seq_length])\n",
        "    alignment_cum = tf.zeros(shape= [batch_size, seq_length])\n",
        "    context = tf.zeros(shape= [batch_size, self.embedding_dim])\n",
        "\n",
        "    return {\n",
        "        \"lstm_outputs\" : lstm_outputs,\n",
        "        \"lstm_hiddens\" : lstm_hiddens,\n",
        "        \"alignment\" : alignment,\n",
        "        \"alignment_cum\" : alignment_cum,\n",
        "        \"context\" : context\n",
        "    }\n",
        "\n",
        "  def parse_decoder_outputs(self, mel_outputs: list, stop_outputs: list, alignment: list):\n",
        "    stop_outputs = tf.transpose(tf.stack(stop_outputs), perm= [0,0,1])\n",
        "    alignment = tf.transpose(tf.stack(alignment), perm= [0,0,1])\n",
        "\n",
        "    mel_outputs = tf.transpose(tf.stack(mel_outputs), perm= [0,0,1])\n",
        "    mel_outputs = tf.reshape(mel_outputs, shape = [mel_outputs.shape[0], -1, self.num_mel_bins])\n",
        "    mel_outputs = tf.transpose(mel_outputs, perm= [0,1,2])\n",
        "\n",
        "    return {\n",
        "        \"mel_outputs\" : mel_outputs,\n",
        "        \"stop_outputs\" : stop_outputs,\n",
        "        \"alignments\" : alignment\n",
        "    }\n",
        "\n",
        "  def forward_step(self, input_var,\n",
        "                   encoder_outputs,\n",
        "                   lstm_outputs,\n",
        "                   lstm_hiddens,\n",
        "                   alignment,\n",
        "                   alignment_cum,\n",
        "                   context) -> None:\n",
        "\n",
        "\n",
        "   input_var = tf.squeeze(input_var, 1)\n",
        "   input_var = tf.concat([input_var, context], axis= -1)\n",
        "\n",
        "   lstm_outputs[1], lstm_hiddens[1] = self.lstm[0](input_var, (lstm_outputs[0], lstm_hiddens[0]))\n",
        "   lstm_outputs[1] = tf.nn.dropout(lstm_outputs[1], rate= self.decoder_dropout_p)\n",
        "\n",
        "   concated_alignment = tf.concat([tf.expand_dims(alignment, 1), tf.expand_dims(alignment_cum, 1)], 1)\n",
        "   context, alignment = self.attention(lstm_outputs[0], encoder_outputs, concated_alignment)\n",
        "   alignment_cum += alignment\n",
        "\n",
        "   output = tf.concat([lstm_hiddens[1], context], axis= -1)\n",
        "\n",
        "   mel_output = self.mel_generator(output)\n",
        "   stop_output = self.stop_generator(output)\n",
        "\n",
        "   return {\n",
        "       \"mel_output\" : mel_output,\n",
        "       \"stop_output\" : stop_output,\n",
        "       \"alignment\" : alignment,\n",
        "       \"alignment_cum\" : alignment_cum,\n",
        "       \"context\" : context,\n",
        "       \"lstm_outputs\" : lstm_outputs,\n",
        "       \"lstm_hiddens\" : lstm_hiddens\n",
        "   }\n",
        "\n",
        "  def call(self,\n",
        "              encoder_outputs,\n",
        "              inputs,\n",
        "              teacher_forcing_ratio: float = 1.0) -> None:\n",
        "\n",
        "    \n",
        "    mel_outputs, stop_outputs, alignments = list(), list(), list()\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "    print(encoder_outputs.shape)\n",
        "    inputs, max_decoding_step = self.validate_args(encoder_outputs, inputs, teacher_forcing_ratio)\n",
        "    decoder_states = self._init_decoder_states(encoder_outputs)\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "      inputs = self.prenet(inputs)\n",
        "\n",
        "      for di in range(max_decoding_step):\n",
        "        input_var = tf.expand_dims(inputs[:, di, :], 1)\n",
        "        decoder_states = self.forward_step(\n",
        "            input_var = input_var,\n",
        "            encoder_outputs = encoder_outputs,\n",
        "            lstm_outputs = decoder_states[\"lstm_outputs\"],\n",
        "            lstm_hiddens = decoder_states[\"lstm_hiddens\"],\n",
        "            alignment = decoder_states[\"alignment\"],\n",
        "            alignment_cum = decoder_states[\"alignment_cum\"],\n",
        "            context = decoder_states[\"context\"]\n",
        "        )\n",
        "\n",
        "        mel_outputs.append(decoder_states[\"mel_outputs\"])\n",
        "        stop_outputs.append(decoder_states[\"stop_output\"])\n",
        "        alignments.append(decoder_states[\"alignment\"])\n",
        "\n",
        "    \n",
        "    else:\n",
        "      input_var = inputs\n",
        "\n",
        "      for di in range(max_decoding_step):\n",
        "        input_var = self.prenet(input_var)\n",
        "        decoder_states = self.forward_step(\n",
        "            input_var = input_var,\n",
        "            encoder_outputs = encoder_outputs,\n",
        "            lstm_outputs = decoder_states[\"lstm_outputs\"],\n",
        "            lstm_hiddens = decoder_states[\"lstm_hiddens\"],\n",
        "            alignment = decoder_states[\"alignment\"],\n",
        "            alignment_cum = decoder_states[\"alignment_cum\"],\n",
        "            context = decoder_states[\"context\"]\n",
        "        )\n",
        "\n",
        "        \n",
        "        mel_outputs.append(decoder_states[\"mel_output\"])\n",
        "        stop_outputs.append(decoder_states[\"stop_output\"])\n",
        "        alignments.append(decoder_states[\"alignment\"])\n",
        "\n",
        "        if tf.math.sigmoid(decoder_states[\"stop_output\"]) > self.stop_threshold:\n",
        "          break\n",
        "        \n",
        "        input_var = decoder_states[\"mel_output\"]\n",
        "    return self.parse_decoder_outputs(mel_outputs, stop_outputs, alignments)\n",
        "\n",
        "\n",
        "  def validate_args(self,\n",
        "                   encoder_outputs,\n",
        "                   inputs,\n",
        "                   teacher_forcing_ratio: float = 1.0):\n",
        "    \n",
        "\n",
        "    #assert encoder_outputs in not None\n",
        "\n",
        "    batch_size = encoder_outputs.shape[0]\n",
        "\n",
        "    if input is None:\n",
        "\n",
        "      inputs = tf.zeros(shape= [batch_size, self.num_mel_bins])\n",
        "      max_decoding_step = self.max_decoding_step\n",
        "\n",
        "      if teacher_forcing_ratio > 0:\n",
        "        raise ValueError(\"Teacher forcing has to be disabled\")\n",
        "\n",
        "    else:\n",
        "      go_frame = tf.expand_dims(tf.zeros(shape= [batch_size, self.num_mel_bins]), 1)\n",
        "      print(batch_size, inputs.shape)\n",
        "      inputs = tf.reshape(inputs, shape = [batch_size, inputs.shape[1], -1])\n",
        "\n",
        "\n",
        "      inputs =tf.concat([go_frame, inputs], axis= 1)\n",
        "      max_decoding_step = inputs.shape[1] - 1\n",
        "\n",
        "    return inputs, max_decoding_step\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPLPVF4sGCYM"
      },
      "source": [
        "batch_size = 3\n",
        "input_seq_length = 10\n",
        "output_seq_length = 100\n",
        "encoder_embedding_dim = 512\n",
        "n_mels = 80\n",
        "\n",
        "encoder_outputs = tf.random.uniform(shape= (batch_size, input_seq_length, encoder_embedding_dim), minval= -0.1, maxval= 0.1)\n",
        "decoder_inputs = tf.random.uniform(shape= (batch_size, output_seq_length, n_mels), minval= -0.1, maxval= 0.1)\n",
        "\n",
        "decoder = Decoder()\n",
        "output = decoder(encoder_outputs, decoder_inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1V02HZ584XQ"
      },
      "source": [
        "class PostNet(tf.keras.Model):\n",
        "  def __init__(self,\n",
        "               num_mel_bins: int = 80,\n",
        "               postnet_dim: int = 512,\n",
        "               num_conv_layers: int = 3,\n",
        "               kernel_size: int = 5,\n",
        "               dropout_p: float = 0.5):\n",
        "    \n",
        "    super(PostNet, self).__init__()\n",
        "\n",
        "    self.conv_layers = list()\n",
        "    self.conv_layers.append(\n",
        "        ConvBlock(\n",
        "            input_dim = postnet_dim,\n",
        "            output_dim = postnet_dim,\n",
        "            kernel_size = kernel_size,\n",
        "            padding = 'valid',\n",
        "            dropout_p = dropout_p,\n",
        "            activation = 'relu' # must -> tanh\n",
        "      ))\n",
        "    \n",
        "    for _ in range(num_conv_layers - 2):\n",
        "      self.conv_layers.append(\n",
        "        ConvBlock(\n",
        "            input_dim = postnet_dim,\n",
        "            output_dim = postnet_dim,\n",
        "            kernel_size = kernel_size,\n",
        "            padding = 'valid',\n",
        "            dropout_p = dropout_p,\n",
        "            activation = 'relu' # must -> tanh\n",
        "      ))\n",
        "\n",
        "\n",
        "    self.conv_layers.append(\n",
        "        ConvBlock(\n",
        "            input_dim = postnet_dim,\n",
        "            output_dim = postnet_dim,\n",
        "            kernel_size = kernel_size,\n",
        "            padding = 'valid',\n",
        "            dropout_p = dropout_p,\n",
        "            activation = 'relu' # must -> tanh\n",
        "      ))\n",
        "    \n",
        "\n",
        "    def call(self, x):\n",
        "      for conv_layer in self.conv_layers:\n",
        "        x = conv_layer(x)\n",
        "\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ch5KyAT31gIV"
      },
      "source": [
        "class Tacotron2(tf.keras.Model):\n",
        "  def __init__(self, args) -> None:\n",
        "    super(Tacotron2, self).__init__()\n",
        "    self.encoder = Encoder(\n",
        "        vocab_size = args.vocab_size,\n",
        "        embedding_dim = args.embedding_dim,\n",
        "        encoder_lstm_dim = args.encoder_lstm_dim,\n",
        "        num_lstm_layers = args.num_encoder_lstm_layers,\n",
        "        conv_dropout_p =args.conv_dropout_p,\n",
        "        num_conv_layers = args.num_encoder_conv_layers,\n",
        "        conv_kernel_size = args.encoder_conv_kernel_size\n",
        "    )\n",
        "\n",
        "    self.decoder = Decoder(      \n",
        "        num_mel_bins = args.num_mel_bins,\n",
        "        prenet_dim = args.prenet_dim,\n",
        "        decoder_lstm_dim = args.decoder_lstm_dim,\n",
        "        attn_lstm_dim = args.attn_lstm_dim,\n",
        "        embedding_dim = args.embedding_dim,\n",
        "        attn_dim = args.attn_dim,\n",
        "        location_conv_filter_size = args.location_conv_filter_size,\n",
        "        location_conv_kernel_size = args.location_conv_kernel_size,\n",
        "        prenet_dropout_p = args.prenet_dropout_p,\n",
        "        attn_dropout_p = args.attn_dropout_p,\n",
        "        decoder_dropout_p = args.decoder_dropout_p,\n",
        "        max_decoding_step = args.max_decoding_step,\n",
        "        stop_threshold = args.stop_threshold\n",
        "    )\n",
        "\n",
        "\n",
        "    self.postnet = PostNet(\n",
        "        num_mel_bins = args.num_mel_bins,\n",
        "        postnet_dim = args.postnet_dim,\n",
        "        num_conv_layers = args.num_postnet_conv_layers,\n",
        "        kernel_size = args.postnet_conv_kernel_size,\n",
        "        dropout_p = args.postnet_dropout_p\n",
        "\n",
        "    )\n",
        "\n",
        "  def call(self,\n",
        "           inputs,\n",
        "           input_lenghts,\n",
        "           targets,\n",
        "           teacher_forcing_ratio: float = 1.0):\n",
        "    \n",
        "    enocder_outputs = self.encoder(inputs, input_lenghts)\n",
        "    decoder_outputs = self.decoder(enocder_outputs, targets, teacher_forcing_ratio)\n",
        "\n",
        "\n",
        "    postnet_outputs = self.postnet(decoder_outputs[\"mel_outputs\"])\n",
        "    decoder_outputs[\"mel_outputs\"] += postnet_outputs\n",
        "\n",
        "\n",
        "    return decoder_outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-h5HmT51g5d"
      },
      "source": [
        "class DefaultArgument:\n",
        "  def __init__(self):\n",
        "    # encoder arguments\n",
        "    self.vocab_size = 10\n",
        "    self.embedding_dim = 512\n",
        "    self.encoder_lstm_dim = 256\n",
        "    self.num_encoder_lstm_layers = 1\n",
        "    self.conv_dropout_p = 0.5\n",
        "    self.num_encoder_conv_layers = 3\n",
        "    self.encoder_conv_kernel_size = 5\n",
        "\n",
        "\n",
        "    # decoder arguments\n",
        "    self.num_mel_bins = 80\n",
        "    self.prenet_dim = 256\n",
        "    self.decoder_lstm_dim = 1024\n",
        "    self.attn_lstm_dim = 1024\n",
        "    self.attn_dim = 128\n",
        "    self.location_conv_filter_size = 32\n",
        "    self.location_conv_kernel_size = 31\n",
        "    self.prenet_dropout_p = 0.5\n",
        "    self.attn_dropout_p = 0.1\n",
        "    self.decoder_dropout_p = 0.1\n",
        "    self.max_decoding_step = 1000\n",
        "    self.stop_threshold = 0.5\n",
        "\n",
        "    # postnet arguments\n",
        "    self.postnet_dim = 512\n",
        "    self.num_postnet_conv_layers = 5\n",
        "    self.postnet_conv_kernel_size = 5\n",
        "    self.postnet_dropout_p = 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTv9xRUUTElo"
      },
      "source": [
        "batch_size = 3\n",
        "seq_length = 3\n",
        "\n",
        "inputs = tf.constant(np.arange(batch_size * seq_length).reshape(batch_size, seq_length))\n",
        "input_lengths = tf.constant([3, 3, 2])\n",
        "targets = tf.random.uniform(shape= (batch_size, 100, 80), minval= -0.1, maxval= 0.1)\n",
        "\n",
        "\n",
        "args = DefaultArgument()\n",
        "model = Tacotron2(args)\n",
        "output = model(inputs, input_lengths, targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yN0viMm5FQ2n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhT_NeojFS2H"
      },
      "source": [
        "import torch\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "me9BCkXJFVxV",
        "outputId": "1aefddca-a592-44d5-eb12-58df496b5fa8"
      },
      "source": [
        "a = torch.range(1, 16)\n",
        "torch.equal(a.view(4,4,1), a.view(4,4,-1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5jdA0mkHFcS"
      },
      "source": [
        "tf.random.uniform(shape= (3,100, 80), minval= -0.1, maxval= 0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEcNSksFHWEm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}